{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8R6Eq0ccXR7",
        "outputId": "0d170c3a-871a-4d0a-828c-a56cea950784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "France, Britain, Russia, and later the United States\n"
          ]
        }
      ],
      "source": [
        "from docx import Document\n",
        "from pptx import Presentation\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import pipeline\n",
        "import pdfplumber\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "\n",
        "# 1. Extract text from different file types\n",
        "def scrape_file(filepath):\n",
        "    if filepath.endswith(\".docx\"):\n",
        "        doc = Document(filepath)\n",
        "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "\n",
        "    elif filepath.endswith(\".pptx\"):\n",
        "        prs = Presentation(filepath)\n",
        "        full_text = []\n",
        "        for slide in prs.slides:\n",
        "            for shape in slide.shapes:\n",
        "                if hasattr(shape, \"text_frame\") and shape.text_frame:\n",
        "                    full_text.append(shape.text_frame.text)\n",
        "        return \"\\n\".join(full_text)\n",
        "\n",
        "    elif filepath.endswith(\".html\"):\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='utf-8') as file:\n",
        "                html_content = file.read()\n",
        "            soup = BeautifulSoup(html_content, 'html.parser')\n",
        "            return soup.get_text(separator=\" \", strip=True)\n",
        "        except Exception as e:\n",
        "            return f\"Error processing HTML file: {e}\"\n",
        "\n",
        "    elif filepath.endswith(\".pdf\"):\n",
        "        try:\n",
        "            with pdfplumber.open(filepath) as pdf:\n",
        "                return \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
        "        except Exception as e:\n",
        "            return f\"Error processing PDF file: {e}\"\n",
        "\n",
        "    elif filepath.endswith(\".png\") or filepath.endswith(\".jpg\") or filepath.endswith(\".jpeg\"):\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(filepath))\n",
        "            return text.strip()\n",
        "        except Exception as e:\n",
        "            return f\"Error processing image file: {e}\"\n",
        "\n",
        "    elif filepath.endswith(\".txt\"):\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='utf-8') as file:\n",
        "                return file.read()\n",
        "        except Exception as e:\n",
        "            return f\"Error processing TXT file: {e}\"\n",
        "\n",
        "    else:\n",
        "        return \"Unsupported file type.\"\n",
        "\n",
        "\n",
        "# 2. Saves extracted text as a normalized JSON file in the output folder\n",
        "def save_extracted_text(filepath, extracted_text):\n",
        "    filename = os.path.basename(filepath).split('.')[0] + \".json\"  # Convert filename\n",
        "    output_path = os.path.join(output_folder, filename)\n",
        "\n",
        "    data = {\"file\": filepath, \"content\": extracted_text}  # JSON structure\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"Saved: {output_path}\")\n",
        "\n",
        "# 3. Load the QA model\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "\n",
        "# 4. Function to answer questions based on extracted text\n",
        "def answer_question(filepath, question):\n",
        "    extracted_text = scrape_file(filepath)\n",
        "\n",
        "    if extracted_text.startswith(\"Error\") or extracted_text == \"Unsupported file type.\":\n",
        "        return extracted_text\n",
        "\n",
        "    # Keep only the first 512 words to fit into the model's context window\n",
        "    context = \" \".join(extracted_text.split()[:512])\n",
        "\n",
        "    response = qa_pipeline(question=question, context=context)\n",
        "    return response[\"answer\"]\n",
        "\n",
        "filepath = \"WorldWar.pdf\"\n",
        "extracted_text = scrape_file(filepath)\n",
        "save_extracted_text(filepath, extracted_text)\n",
        "question = \"What were the main causes of World War I?\"\n",
        "answer = answer_question(filepath, question)\n",
        "print(answer)"
      ]
    }
  ]
}